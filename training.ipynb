{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "**about batch**\n",
    "\n",
    "> Since there's a finite amount of comments, it is much easier to build a batch based on target keyword features. Here we fetch three different databases sequentially (Duplicates if any, are removed automatically).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Fit the batch of queries to a training dataset. It's easy to load from any data source, as long as it's of type -> `Iterable[List[str]]` and use the text preprocessing methods available in the `david_sentiment.dataset` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from david_sentiment import YTCSentimentConfig, YTCSentimentModel\n",
    "import david_sentiment.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1483/1483 [00:00<00:00, 14271.51/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3mâš  * Preprocessing batch with 1483 samples...\u001b[0m\n",
      "\u001b[38;5;2mâœ” * Removed 46 comments  from 1483. Returning size: 1437\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Docs: 0 [00:00, ?/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3mâš  * Transforming texts to sentences with en_core_web_sm model.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 1437 [00:05, 262.38/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” * Done! Sucessfully preprocessed 3141 sentences.\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ * Size before: 1437, and after: 3141.\u001b[0m\n",
      "\u001b[38;5;3mâš  * Building to training data...\u001b[0m\n",
      "\u001b[38;5;2mâœ” * Done! x_train: 1132, x_labels: 1132.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = YTCSentimentConfig(max_strlen=3000, project_dir=\"ytc_sentiment\")\n",
    "\n",
    "batch = ds.BatchDB([\n",
    "    ds.Fetch('unbox', \"%make a video%\"),\n",
    "    ds.Fetch('v1', \"%make a video%\"),\n",
    "])\n",
    "x_train, x_labels, y_test = ds.fit_batch_to_dataset(batch, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "> That's it! we can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Loading vocab file from\n",
      "/home/ego/david_models/glove/glove.6B/glove.6B.100d.txt\u001b[0m\n",
      "\u001b[38;5;2mâœ” num-dim:(100), vocab-size: 2552\u001b[0m\n",
      "\u001b[38;5;2mâœ” *** embedding vocabulary ğŸ¤— ***\u001b[0m\n",
      "Epoch 1/100\n",
      "1132/1132 [==============================] - 1s 870us/step - loss: 0.6139 - acc: 0.7120\n",
      "Epoch 2/100\n",
      "1132/1132 [==============================] - 1s 682us/step - loss: 0.5320 - acc: 0.7447\n",
      "Epoch 3/100\n",
      "1132/1132 [==============================] - 1s 727us/step - loss: 0.4891 - acc: 0.7615\n",
      "Epoch 4/100\n",
      "1132/1132 [==============================] - 1s 795us/step - loss: 0.4536 - acc: 0.7959\n",
      "Epoch 5/100\n",
      "1132/1132 [==============================] - 1s 625us/step - loss: 0.4271 - acc: 0.8030\n",
      "Epoch 6/100\n",
      "1132/1132 [==============================] - 1s 744us/step - loss: 0.4040 - acc: 0.8242\n",
      "Epoch 7/100\n",
      "1132/1132 [==============================] - 1s 666us/step - loss: 0.3860 - acc: 0.8339\n",
      "Epoch 8/100\n",
      "1132/1132 [==============================] - 1s 587us/step - loss: 0.3641 - acc: 0.8507\n",
      "Epoch 9/100\n",
      "1132/1132 [==============================] - 1s 690us/step - loss: 0.3474 - acc: 0.8613\n",
      "Epoch 10/100\n",
      "1132/1132 [==============================] - 1s 698us/step - loss: 0.3344 - acc: 0.8701\n",
      "Epoch 11/100\n",
      "1132/1132 [==============================] - 1s 719us/step - loss: 0.3213 - acc: 0.8754\n",
      "Epoch 12/100\n",
      "1132/1132 [==============================] - 1s 655us/step - loss: 0.3061 - acc: 0.8896\n",
      "Epoch 13/100\n",
      "1132/1132 [==============================] - 1s 667us/step - loss: 0.2943 - acc: 0.8905\n",
      "Epoch 14/100\n",
      "1132/1132 [==============================] - 1s 757us/step - loss: 0.2855 - acc: 0.9037\n",
      "Epoch 15/100\n",
      "1132/1132 [==============================] - 1s 744us/step - loss: 0.2775 - acc: 0.9019\n",
      "Epoch 16/100\n",
      "1132/1132 [==============================] - 1s 699us/step - loss: 0.2657 - acc: 0.9152\n",
      "Epoch 17/100\n",
      "1132/1132 [==============================] - 1s 637us/step - loss: 0.2587 - acc: 0.9267\n",
      "Epoch 18/100\n",
      "1132/1132 [==============================] - 1s 546us/step - loss: 0.2495 - acc: 0.9302\n",
      "Epoch 19/100\n",
      "1132/1132 [==============================] - 1s 565us/step - loss: 0.2429 - acc: 0.9435\n",
      "Epoch 20/100\n",
      "1132/1132 [==============================] - 1s 832us/step - loss: 0.2350 - acc: 0.9329\n",
      "Epoch 21/100\n",
      "1132/1132 [==============================] - 1s 771us/step - loss: 0.2294 - acc: 0.9496\n",
      "Epoch 22/100\n",
      "1132/1132 [==============================] - 1s 660us/step - loss: 0.2228 - acc: 0.9417\n",
      "Epoch 23/100\n",
      "1132/1132 [==============================] - 1s 682us/step - loss: 0.2159 - acc: 0.9523\n",
      "Epoch 24/100\n",
      "1132/1132 [==============================] - 1s 844us/step - loss: 0.2099 - acc: 0.9567\n",
      "Epoch 25/100\n",
      "1132/1132 [==============================] - 1s 817us/step - loss: 0.2056 - acc: 0.9488\n",
      "Epoch 26/100\n",
      "1132/1132 [==============================] - 1s 767us/step - loss: 0.2005 - acc: 0.9505\n",
      "Epoch 27/100\n",
      "1132/1132 [==============================] - 1s 874us/step - loss: 0.1945 - acc: 0.9567\n",
      "Epoch 28/100\n",
      "1132/1132 [==============================] - 1s 629us/step - loss: 0.1938 - acc: 0.9620\n",
      "Epoch 29/100\n",
      "1132/1132 [==============================] - 1s 731us/step - loss: 0.1862 - acc: 0.9594\n",
      "Epoch 30/100\n",
      "1132/1132 [==============================] - 1s 777us/step - loss: 0.1818 - acc: 0.9629\n",
      "Epoch 31/100\n",
      "1132/1132 [==============================] - 1s 792us/step - loss: 0.1778 - acc: 0.9611\n",
      "Epoch 32/100\n",
      "1132/1132 [==============================] - 1s 813us/step - loss: 0.1733 - acc: 0.9655\n",
      "Epoch 33/100\n",
      "1132/1132 [==============================] - 1s 670us/step - loss: 0.1705 - acc: 0.9655\n",
      "Epoch 34/100\n",
      "1132/1132 [==============================] - 1s 602us/step - loss: 0.1679 - acc: 0.9682\n",
      "Epoch 35/100\n",
      "1132/1132 [==============================] - 1s 753us/step - loss: 0.1636 - acc: 0.9673\n",
      "Epoch 36/100\n",
      "1132/1132 [==============================] - 1s 612us/step - loss: 0.1595 - acc: 0.9700\n",
      "Epoch 37/100\n",
      "1132/1132 [==============================] - 1s 672us/step - loss: 0.1560 - acc: 0.9691\n",
      "Epoch 38/100\n",
      "1132/1132 [==============================] - 1s 584us/step - loss: 0.1529 - acc: 0.9691\n",
      "Epoch 39/100\n",
      "1132/1132 [==============================] - 1s 577us/step - loss: 0.1496 - acc: 0.9744\n",
      "Epoch 40/100\n",
      "1132/1132 [==============================] - 1s 656us/step - loss: 0.1470 - acc: 0.9726\n",
      "Epoch 41/100\n",
      "1132/1132 [==============================] - 1s 673us/step - loss: 0.1439 - acc: 0.9708\n",
      "Epoch 42/100\n",
      "1132/1132 [==============================] - 1s 696us/step - loss: 0.1412 - acc: 0.9761\n",
      "Epoch 43/100\n",
      "1132/1132 [==============================] - 1s 574us/step - loss: 0.1380 - acc: 0.9753\n",
      "Epoch 44/100\n",
      "1132/1132 [==============================] - 1s 764us/step - loss: 0.1361 - acc: 0.9717\n",
      "Epoch 45/100\n",
      "1132/1132 [==============================] - 1s 720us/step - loss: 0.1334 - acc: 0.9779\n",
      "Epoch 46/100\n",
      "1132/1132 [==============================] - 1s 817us/step - loss: 0.1306 - acc: 0.9761\n",
      "Epoch 47/100\n",
      "1132/1132 [==============================] - 1s 725us/step - loss: 0.1286 - acc: 0.9770\n",
      "Epoch 48/100\n",
      "1132/1132 [==============================] - 1s 746us/step - loss: 0.1258 - acc: 0.9779\n",
      "Epoch 49/100\n",
      "1132/1132 [==============================] - 1s 518us/step - loss: 0.1241 - acc: 0.9779\n",
      "Epoch 50/100\n",
      "1132/1132 [==============================] - 1s 599us/step - loss: 0.1230 - acc: 0.9761\n",
      "Epoch 51/100\n",
      "1132/1132 [==============================] - 1s 596us/step - loss: 0.1192 - acc: 0.9797\n",
      "Epoch 52/100\n",
      "1132/1132 [==============================] - 1s 768us/step - loss: 0.1190 - acc: 0.9814\n",
      "Epoch 53/100\n",
      "1132/1132 [==============================] - 1s 544us/step - loss: 0.1156 - acc: 0.9806\n",
      "Epoch 54/100\n",
      "1132/1132 [==============================] - 1s 587us/step - loss: 0.1136 - acc: 0.9797\n",
      "Epoch 55/100\n",
      "1132/1132 [==============================] - 1s 677us/step - loss: 0.1113 - acc: 0.9814\n",
      "Epoch 56/100\n",
      "1132/1132 [==============================] - 1s 725us/step - loss: 0.1096 - acc: 0.9814\n",
      "Epoch 57/100\n",
      "1132/1132 [==============================] - 1s 704us/step - loss: 0.1080 - acc: 0.9823\n",
      "Epoch 58/100\n",
      "1132/1132 [==============================] - 1s 795us/step - loss: 0.1056 - acc: 0.9841\n",
      "Epoch 59/100\n",
      "1132/1132 [==============================] - 1s 648us/step - loss: 0.1040 - acc: 0.9823\n",
      "Epoch 60/100\n",
      "1132/1132 [==============================] - 1s 735us/step - loss: 0.1027 - acc: 0.9850\n",
      "Epoch 61/100\n",
      "1132/1132 [==============================] - 1s 645us/step - loss: 0.1019 - acc: 0.9832\n",
      "Epoch 62/100\n",
      "1132/1132 [==============================] - 1s 597us/step - loss: 0.1008 - acc: 0.9841\n",
      "Epoch 63/100\n",
      "1132/1132 [==============================] - 1s 526us/step - loss: 0.0973 - acc: 0.9850\n",
      "Epoch 64/100\n",
      "1132/1132 [==============================] - 1s 461us/step - loss: 0.0960 - acc: 0.9867\n",
      "Epoch 65/100\n",
      "1132/1132 [==============================] - 0s 431us/step - loss: 0.0950 - acc: 0.9850\n",
      "Epoch 66/100\n",
      "1132/1132 [==============================] - 1s 506us/step - loss: 0.0937 - acc: 0.9867\n",
      "Epoch 67/100\n",
      "1132/1132 [==============================] - 1s 735us/step - loss: 0.0914 - acc: 0.9859\n",
      "Epoch 68/100\n",
      "1132/1132 [==============================] - 1s 729us/step - loss: 0.0900 - acc: 0.9867\n",
      "Epoch 69/100\n",
      "1132/1132 [==============================] - 1s 651us/step - loss: 0.0887 - acc: 0.9885\n",
      "Epoch 70/100\n",
      "1132/1132 [==============================] - 1s 656us/step - loss: 0.0874 - acc: 0.9894\n",
      "Epoch 71/100\n",
      "1132/1132 [==============================] - 1s 577us/step - loss: 0.0861 - acc: 0.9867\n",
      "Epoch 72/100\n",
      "1132/1132 [==============================] - 1s 596us/step - loss: 0.0845 - acc: 0.9885\n",
      "Epoch 73/100\n",
      "1132/1132 [==============================] - 1s 609us/step - loss: 0.0831 - acc: 0.9885\n",
      "Epoch 74/100\n",
      "1132/1132 [==============================] - 1s 621us/step - loss: 0.0823 - acc: 0.9885\n",
      "Epoch 75/100\n",
      "1132/1132 [==============================] - 1s 592us/step - loss: 0.0809 - acc: 0.9903\n",
      "Epoch 76/100\n",
      "1132/1132 [==============================] - 1s 652us/step - loss: 0.0795 - acc: 0.9894\n",
      "Epoch 77/100\n",
      "1132/1132 [==============================] - 1s 778us/step - loss: 0.0786 - acc: 0.9903\n",
      "Epoch 78/100\n",
      "1132/1132 [==============================] - 1s 750us/step - loss: 0.0775 - acc: 0.9912\n",
      "Epoch 79/100\n",
      "1132/1132 [==============================] - 1s 727us/step - loss: 0.0767 - acc: 0.9929\n",
      "Epoch 80/100\n",
      "1132/1132 [==============================] - 1s 686us/step - loss: 0.0753 - acc: 0.9947\n",
      "Epoch 81/100\n",
      "1132/1132 [==============================] - 1s 666us/step - loss: 0.0739 - acc: 0.9920\n",
      "Epoch 82/100\n",
      "1132/1132 [==============================] - 1s 582us/step - loss: 0.0731 - acc: 0.9947\n",
      "Epoch 83/100\n",
      "1132/1132 [==============================] - 1s 741us/step - loss: 0.0728 - acc: 0.9938\n",
      "Epoch 84/100\n",
      "1132/1132 [==============================] - 1s 701us/step - loss: 0.0709 - acc: 0.9938\n",
      "Epoch 85/100\n",
      "1132/1132 [==============================] - 1s 683us/step - loss: 0.0695 - acc: 0.9965\n",
      "Epoch 86/100\n",
      "1132/1132 [==============================] - 1s 700us/step - loss: 0.0691 - acc: 0.9920\n",
      "Epoch 87/100\n",
      "1132/1132 [==============================] - 1s 578us/step - loss: 0.0675 - acc: 0.9947\n",
      "Epoch 88/100\n",
      "1132/1132 [==============================] - 1s 526us/step - loss: 0.0672 - acc: 0.9929\n",
      "Epoch 89/100\n",
      "1132/1132 [==============================] - 1s 847us/step - loss: 0.0660 - acc: 0.9956\n",
      "Epoch 90/100\n",
      "1132/1132 [==============================] - 1s 793us/step - loss: 0.0652 - acc: 0.9956\n",
      "Epoch 91/100\n",
      "1132/1132 [==============================] - 1s 638us/step - loss: 0.0646 - acc: 0.9956\n",
      "Epoch 92/100\n",
      "1132/1132 [==============================] - 1s 581us/step - loss: 0.0631 - acc: 0.9956\n",
      "Epoch 93/100\n",
      "1132/1132 [==============================] - 1s 562us/step - loss: 0.0627 - acc: 0.9956\n",
      "Epoch 94/100\n",
      "1132/1132 [==============================] - 1s 548us/step - loss: 0.0612 - acc: 0.9956\n",
      "Epoch 95/100\n",
      "1132/1132 [==============================] - 1s 656us/step - loss: 0.0609 - acc: 0.9947\n",
      "Epoch 96/100\n",
      "1132/1132 [==============================] - 1s 782us/step - loss: 0.0596 - acc: 0.9965\n",
      "Epoch 97/100\n",
      "1132/1132 [==============================] - 1s 739us/step - loss: 0.0594 - acc: 0.9973\n",
      "Epoch 98/100\n",
      "1132/1132 [==============================] - 1s 607us/step - loss: 0.0577 - acc: 0.9973\n",
      "Epoch 99/100\n",
      "1132/1132 [==============================] - 1s 774us/step - loss: 0.0572 - acc: 0.9973\n",
      "Epoch 100/100\n",
      "1132/1132 [==============================] - 1s 798us/step - loss: 0.0566 - acc: 0.9973\n"
     ]
    }
   ],
   "source": [
    "ytc_sentiment = YTCSentimentModel(config)\n",
    "ytc_sentiment.train_model(x_train, x_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demos\n",
    "\n",
    "**`test_polarity_attention_weights()` + `test_unseen_samples()`**\n",
    "\n",
    "While its a rather simple test and not an **official** technique to evalue an embedding model. It does helps at detecting (based on own judgement) the balance between the sum of N `P(negative|positive)` token sequences:\n",
    "\n",
    "> When scores are: *high* -> *low* : `Balanced-model`\n",
    "\n",
    "```python\n",
    "input: \"I hate this, but love it :)\" < pos(ğŸ˜Š) (65.7528)% >\n",
    "input: \"I love this, but hate it :)\" < pos(ğŸ˜Š) (63.7456)% >\n",
    "input: \"I hate this, but love it :(\" < neg(ğŸ˜‘) (54.1849)% >\n",
    "input: \"I love this, but hate it :(\" < neg(ğŸ˜¶) (51.9947)% >\n",
    "```\n",
    "\n",
    "- Others (Based on experience):\n",
    "    - Score values have no order (all-over) : `Overfitted-model`\n",
    "    - Positive texts are labeled as negative : `Bias-samples`\n",
    "    \n",
    "> ***TIP*** kewords like `love, hate` or similar increase bias or/and overfitting. Intead try fetching topics, titles e.g., `%make a video%`, where the word-tokens are not based purely on `sentimentalism`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from david_sentiment.utils import test_polarity_attention_weights, test_unseen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"I hate this, but love it :)\" < pos(ğŸ˜Š) (65.7528)% >\n",
      "input: \"I love this, but hate it :)\" < pos(ğŸ˜Š) (63.7456)% >\n",
      "input: \"I hate this, but love it :(\" < neg(ğŸ˜‘) (54.1849)% >\n",
      "input: \"I love this, but hate it :(\" < neg(ğŸ˜¶) (51.9947)% >\n"
     ]
    }
   ],
   "source": [
    "# Small demo - test the model's attention weights.\n",
    "test_polarity_attention_weights(ytc_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ (Old=0.0, New=77.6381)\n",
      " ğŸ˜ - m15 /m17 upgraded version.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=73.272)\n",
      " ğŸ˜… - shall weï»¿\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=30.4416)\n",
      " ğŸ˜³ - I thought it was an explanation.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=99.9355)\n",
      " ğŸ˜ - lew make a video on all the pictures your phones taken from u making phone reviews.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=99.3777)\n",
      " ğŸ˜ - A VIDEO OF YOU PLAYING HOCKEY IN BEATS PB PRO\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=94.8878)\n",
      " ğŸ¤— - Thank you\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=83.3552)\n",
      " ğŸ˜ - Can you make a video of iPados beta ?\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=33.9808)\n",
      " ğŸ˜¬ - Windows\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=17.025)\n",
      " ğŸ˜  - what do you feel about switching to the oneplus\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=99.8896)\n",
      " ğŸ˜ - pewdiepie plz u subcribe me and make a video on me\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=91.9985)\n",
      " ğŸ˜€ - You should make a video of you playing PUBG on this phone.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=48.4672)\n",
      " ğŸ˜¶ - If it's supposed to be an april fools\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=98.5463)\n",
      " ğŸ˜ - Plz make a video on India, cost & religion system....\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=52.7096)\n",
      " ğŸ˜‘ - Plz make a video of redmi note 7 pro\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=31.7184)\n",
      " ğŸ˜³ - Go watch it.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=79.7811)\n",
      " ğŸ˜ - make a video covering each accessory please man!\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=95.4761)\n",
      " ğŸ¤— - Make a video on Redmi k20 pro\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=86.7035)\n",
      " ğŸ˜€ - It's about balance.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=95.139)\n",
      " ğŸ¤— - Would you please make a video on Funcl W1 and Funcl AI earphones.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=78.5567)\n",
      " ğŸ˜ - Will you make a video on it ?\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=98.7835)\n",
      " ğŸ˜ - Please think about it and make a video if you can.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=94.1769)\n",
      " ğŸ¤— - we could hope to see in 2020??\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=98.7844)\n",
      " ğŸ˜ - Make a video about not a smartphone plzzzzzzz\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=96.7084)\n",
      " ğŸ¤— - You donâ€™t have to be a bitch.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=47.5426)\n",
      " ğŸ˜¶ - Think about that.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=98.4927)\n",
      " ğŸ˜ - can you make a video on how to make thumbnail.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=1.5344)\n",
      " ğŸ¤¬ - Please make a video about the vivo nex 2! ğŸ™\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=89.943)\n",
      " ğŸ˜€ - Your biggest fan\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=97.6116)\n",
      " ğŸ˜ - Please make a video on how to use Facebook without internet.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=61.0681)\n",
      " ğŸ˜‘ - A BIG DEAL\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=91.3205)\n",
      " ğŸ˜€ - but I use my phone a lot for work and Netflix\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=40.8797)\n",
      " ğŸ˜’ - so why stop.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=97.6973)\n",
      " ğŸ˜ - Health, wealth and mind.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=55.4884)\n",
      " ğŸ˜‘ - Dose\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=84.7458)\n",
      " ğŸ˜ - SO THEY ARENT\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=42.6375)\n",
      " ğŸ˜’ - I would like to know your opinion.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=26.5492)\n",
      " ğŸ˜¤ - Liza donâ€™t believe those hater lovers are here for you\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=88.4663)\n",
      " ğŸ˜€ - make a video on redmi note 7 pro\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=92.3651)\n",
      " ğŸ˜€ - Alright, But make a video of hacking\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=93.6326)\n",
      " ğŸ¤— - Please make a video on oppo reno\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=91.4836)\n",
      " ğŸ˜€ - contacted with my isp\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=73.4306)\n",
      " ğŸ˜… - Does it run fortnite?\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=83.1659)\n",
      " ğŸ˜ - A $360 flagship killer.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=18.7658)\n",
      " ğŸ˜  - IF he didn't like it, he'd probably say so on that too.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=97.7472)\n",
      " ğŸ˜ - Please make a video about what video editor you use.\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=92.3855)\n",
      " ğŸ˜€ - All the videos i watched did not work.ï»¿\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=94.9553)\n",
      " ğŸ¤— - But i gotta make people click and think they are getting the video they are expecting to see...\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=56.9683)\n",
      " ğŸ˜‘ - x please check it out on aliexpress\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=88.8065)\n",
      " ğŸ˜€ - Gtfo!\n",
      "\n",
      "ğŸ’¬ (Old=0.0, New=74.7175)\n",
      " ğŸ˜… - your watch u wearing..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small demo - run predictions over k random test_samples.\n",
    "test_unseen_samples(ytc_sentiment, y_test, print_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"hello, world! i am glad this demo worked! :)\" < pos(ğŸ˜) (98.3824)% >\n"
     ]
    }
   ],
   "source": [
    "# predict sentiment with punctuation.\n",
    "\n",
    "ytc_sentiment.print_predict(\"hello, world! i am glad this demo worked! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"hello world I am glad this demo worked\" < pos(ğŸ˜€) (91.5674)% >\n"
     ]
    }
   ],
   "source": [
    "# predict sentiment without punctuation.\n",
    "\n",
    "ytc_sentiment.print_predict(\"hello world I am glad this demo worked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your session and use later\n",
    "\n",
    "**you wont have to load the dataset again**\n",
    "\n",
    "> The following method creates a new project. The `YTCSentimentConfig` class uses these files re-load your model and tokenizer AKA `your project`. So you can feel free to save what you have now and continue later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytc_sentiment.save_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}